import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from torch.utils.data import Dataset, DataLoader
import os
import torch
import random
from joblib import dump
from icecream import ic


#Importando 201
df1 = pd.read_csv('tls201_part01.csv', sep=';', header=0, index_col=False)
df2 = pd.read_csv('tls201_part02.csv', sep=';', header=0, index_col=False)
df3 = pd.read_csv('tls201_part02.csv', sep=';', header=0, index_col=False)

df_201 = pd.concat([df1, df2, df3])

df_201 = df_201.drop(['appln_nr', 'appln_filing_date', 'appln_filing_year', 'appln_nr_epodoc', 'appln_nr_original', 'internat_appln_id', 'earliest_filing_date', 
'earliest_filing_year', 'earliest_filing_id', 'earliest_publn_date', 
'earliest_pat_publn_id', 'docdb_family_id', 'inpadoc_family_id'], axis=1)

del [df1, df2, df3]

#Importando 209

df4 =
df5 =

df_209_temp = pd.concat([df4, df5])
df_209_temp = df_209_temp.drop(['ipc_version', 'ipc_value', 'ipc_position', 'ipc_gener_auth'])

# Filtro WIPO IA CEIS
df_wipo = pd.read_csv('resultados_patentes.csv')
df_209 = pd.merge(df_wipo, df_209_temp, how = 'inner', on = 'ipc_class_symbol')

#Importando 206

df6 =
df7 =
df_206 = pd.concat([df6, df7])

#Importando 207


#Importando 211

#Importando 212

#Importando Caixa Preta


##########################################
class SensorDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, csv_name, root_dir, training_length, forecast_window):
        """
        Args:
            csv_file (string): Path to the csv file.
            root_dir (string): Directory
        """
        
        # load raw data file
        csv_file = os.path.join(root_dir, csv_name)
        self.df = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = MinMaxScaler()
        self.T = training_length
        self.S = forecast_window

    def __len__(self):
        # return number of sensors
        return len(self.df.groupby(by=["reindexed_id"]))

    # Will pull an index between 0 and __len__. 
    def __getitem__(self, idx):
        
        # Sensors are indexed from 1
        idx = idx+1

        # np.random.seed(0)

        start = np.random.randint(0, len(self.df[self.df["reindexed_id"]==idx]) - self.T - self.S) 
        sensor_number = str(self.df[self.df["reindexed_id"]==idx][["sensor_id"]][start:start+1].values.item())
        index_in = torch.tensor([i for i in range(start, start+self.T)])
        index_tar = torch.tensor([i for i in range(start + self.T, start + self.T + self.S)])
        _input = torch.tensor(self.df[self.df["reindexed_id"]==idx][["humidity", "sin_hour", "cos_hour", "sin_day", "cos_day", "sin_month", "cos_month"]][start : start + self.T].values)
        target = torch.tensor(self.df[self.df["reindexed_id"]==idx][["humidity", "sin_hour", "cos_hour", "sin_day", "cos_day", "sin_month", "cos_month"]][start + self.T : start + self.T + self.S].values)

        # scalar is fit only to the input, to avoid the scaled values "leaking" information about the target range.
        # scalar is fit only for humidity, as the timestamps are already scaled
        # scalar input/output of shape: [n_samples, n_features].
        scaler = self.transform

        scaler.fit(_input[:,0].unsqueeze(-1))
        _input[:,0] = torch.tensor(scaler.transform(_input[:,0].unsqueeze(-1)).squeeze(-1))
        target[:,0] = torch.tensor(scaler.transform(target[:,0].unsqueeze(-1)).squeeze(-1))

        # save the scalar to be used later when inverse translating the data for plotting.
        dump(scaler, 'scalar_item.joblib')

        return index_in, index_tar, _input, target, sensor_number


