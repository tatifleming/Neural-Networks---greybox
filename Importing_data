##alterações
# passos da filtragem

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from torch.utils.data import Dataset, DataLoader
import os
import torch
import random
from joblib import dump
from icecream import ic


#Importando 201
df1 = pd.read_csv('tls201_part01.csv', sep=';', header=0, index_col=False)
df2 = pd.read_csv('tls201_part02.csv', sep=';', header=0, index_col=False)
df3 = pd.read_csv('tls201_part02.csv', sep=';', header=0, index_col=False)

df201 = pd.concat([df1, df2, df3])

df201 = df_201.drop(['appln_nr','appln_nr_epodoc'], axis=1)

del [df1, df2, df3]

#1) removing artificial patents
dfart = df201.dropna(subset=['appln_nr_original'])

## contando quantas W (PCT)
sum(dfart['appln_kind'].str.strip() == 'W')

## alterando tipo de patente para string
dfart['appln_kind'] = dfart['appln_kind'].str.strip()

#2) selecting only appln_kind = A & W
dfkind = dfart[dfart['appln_kind'].isin(['A', 'W'])]

##changing type of the column earliest filing date
dfkind['earliest_filing_date'] = pd.to_datetime(dfkind['earliest_filing_date'], format="%Y-%m-%d")
## test
np.min(dfkind['earliest_filing_date'])

# 3) prioridade de famílias
dfprior1 = dfkind[dfkind["docdb_family_id"]].groupby("earliest_filing_date", group_keys=False).apply(lambda x: x[x['appln_id'].eq(x['appln_id'].min())])

# 4) selecionar anos que nos são de interesse
after_2000_df = df_dois[(df_dois['earliest_publn_year'] >= 2000) & (~df_dois['earliest_publn_year'].isin([9999, 2020]))]

#Importando 209
#Importando 206
#Importando 207

df209_temp = pd.concat([df4, df5])
df209_temp = df_209_temp.drop(['ipc_version', 'ipc_value', 'ipc_position', 'ipc_gener_auth'])

# 5) Filtro WIPO IA CEIS
df_wipo = pd.read_csv('resultados_patentes.csv')
df_209 = pd.merge(df_wipo, df_209_temp, how = 'inner', on = 'ipc_class_symbol')

# 6) Selecionar apenas aplicantes = inventores
# está em spark
df207_ia = df207_invt.where((col(['applt_seq_nr', 'invt_seq_nr']) > 0))))
df207_ia.repartition(1).write.option("header", "true").csv("patstat")

# 7) selecionar apenas as patentes que estão no nível internacional
dfinternacional = df[df['int_phase']=='Y']
df207_ia_int.repartition(1).write.option("header", "true").csv("patstat_int")   

#Importando Caixa Preta

##########################################
class SensorDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, csv_name, root_dir, training_length, forecast_window):
        """
        Args:
            csv_file (string): Path to the csv file.
            root_dir (string): Directory
        """
        
        # load raw data file
        csv_file = os.path.join(root_dir, csv_name)
        self.df = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = MinMaxScaler()
        self.T = training_length
        self.S = forecast_window

    def __len__(self):
        # return number of sensors
        return len(self.df.groupby(by=["reindexed_id"]))

    # Will pull an index between 0 and __len__. 
    def __getitem__(self, idx):
        
        # Sensors are indexed from 1
        idx = idx+1

        # np.random.seed(0)

        start = np.random.randint(0, len(self.df[self.df["reindexed_id"]==idx]) - self.T - self.S) 
        sensor_number = str(self.df[self.df["reindexed_id"]==idx][["sensor_id"]][start:start+1].values.item())
        index_in = torch.tensor([i for i in range(start, start+self.T)])
        index_tar = torch.tensor([i for i in range(start + self.T, start + self.T + self.S)])
        _input = torch.tensor(self.df[self.df["reindexed_id"]==idx][["humidity", "sin_hour", "cos_hour", "sin_day", "cos_day", "sin_month", "cos_month"]][start : start + self.T].values)
        target = torch.tensor(self.df[self.df["reindexed_id"]==idx][["humidity", "sin_hour", "cos_hour", "sin_day", "cos_day", "sin_month", "cos_month"]][start + self.T : start + self.T + self.S].values)

        # scalar is fit only to the input, to avoid the scaled values "leaking" information about the target range.
        # scalar is fit only for humidity, as the timestamps are already scaled
        # scalar input/output of shape: [n_samples, n_features].
        scaler = self.transform

        scaler.fit(_input[:,0].unsqueeze(-1))
        _input[:,0] = torch.tensor(scaler.transform(_input[:,0].unsqueeze(-1)).squeeze(-1))
        target[:,0] = torch.tensor(scaler.transform(target[:,0].unsqueeze(-1)).squeeze(-1))

        # save the scalar to be used later when inverse translating the data for plotting.
        dump(scaler, 'scalar_item.joblib')

        return index_in, index_tar, _input, target, sensor_number


